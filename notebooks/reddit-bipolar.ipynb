{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c2651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clean-nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, ne_chunk, Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a1d9f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a6427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipolar_df = pd.read_csv('../data/RedditBipolar.csv', encoding='latin9', delimiter=';', parse_dates=['date'])\n",
    "bipolar_df = bipolar_df[['date', 'author', 'post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cec643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1692, 3)\n",
      "1692\n",
      "1618\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1692 entries, 0 to 1691\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    1692 non-null   object\n",
      " 1   author  1692 non-null   object\n",
      " 2   post    1692 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 39.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(bipolar_df.shape)\n",
    "print(bipolar_df['post'].nunique())\n",
    "print(bipolar_df['author'].nunique())\n",
    "print(bipolar_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba4d6f",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2ca0c",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipolar_df['post_tokens_uppercase'] = bipolar_df['post'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b2ef2",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['make', 'feel', 'like', 'going', 'thing', 'something', 'get', 'http', 'else', 'anyone', 'bipolar', 'disorder']\n",
    "\n",
    "bipolar_df['post_tokens_lowercase'] = bipolar_df['post_tokens_uppercase'].apply(\n",
    "  lambda x: [word.lower() for word in x if word.lower().isalpha() and word.lower() not in stop_words]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b73e9",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipolar_df['post_tokens_uppercase_with_pos_tags'] = bipolar_df['post_tokens_uppercase'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb311b24",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(tree):\n",
    "  entities = []\n",
    "  for node in tree:\n",
    "    if isinstance(node, Tree): # instance of nameed entity\n",
    "      entity_type = node.label()\n",
    "      entity_words = \" \".join(word for word, _ in node.leaves())\n",
    "      entities.append((entity_words, entity_type))\n",
    "  return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e437ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipolar_df['named_entities'] = bipolar_df['post_tokens_uppercase_with_pos_tags'].apply(\n",
    "    lambda tagged: extract_named_entities(ne_chunk(tagged))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876a270",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcbb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pos_tags(tag: str):\n",
    "  if tag.startswith('J'):\n",
    "      return 'a'\n",
    "  elif tag.startswith('V'):\n",
    "      return 'v'\n",
    "  elif tag.startswith('R'):\n",
    "      return 'r'\n",
    "  return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79dadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "bipolar_df['post_lemmas_lowercase'] = bipolar_df['post_tokens_lowercase'].apply(lambda x: [wnl.lemmatize(word) for word in x])\n",
    "\n",
    "bipolar_df['post_lemmas_uppercase'] = bipolar_df['post_tokens_uppercase_with_pos_tags'].apply(\n",
    "  lambda tagged: [wnl.lemmatize(word=word, pos=map_pos_tags(tag)) for word, tag in tagged]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a69fe",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f3cfb",
   "metadata": {},
   "source": [
    "### Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmas = list(chain.from_iterable(bipolar_df['post_lemmas_lowercase']))\n",
    "word_count = Counter(all_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = word_count.most_common(20)\n",
    "\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(words, counts)\n",
    "plt.title('Top Word Counts')\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f7952",
   "metadata": {},
   "source": [
    "### Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0462a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_count)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Most Frequent Lemmas\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97a198",
   "metadata": {},
   "source": [
    "### Top entities overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = list(chain.from_iterable(bipolar_df['named_entities']))\n",
    "all_entities = [word for word, _ in all_entities]\n",
    "entity_counts = Counter(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(entity_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.title('Named Entity Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a3143",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606535b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipolar_df = bipolar_df.drop(['compound', 'pos', 'neg', 'neu'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# generates a dictionary of normalized scores (neg, neu, pos, compound)\n",
    "bipolar_df['sentiment'] = bipolar_df['post'].apply(sia.polarity_scores)\n",
    "\n",
    "# creates a dataframe from json\n",
    "sentiment_df = pd.json_normalize(bipolar_df['sentiment'])\n",
    "\n",
    "# concatenates the bipolar_df and sentiment_df columns\n",
    "bipolar_df = pd.concat([bipolar_df, sentiment_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bipolar_df['compound'], bins=50)\n",
    "plt.title('Distribution of Compound Sentiment Scores')\n",
    "plt.xlabel('Compound Sentiment Score')\n",
    "plt.ylabel('Post Count')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1651bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(x: float):\n",
    "  if x <= -0.05:\n",
    "    return 'Negative'\n",
    "  if x >= 0.05:\n",
    "    return 'Positive'\n",
    "  return 'Neutral'\n",
    "\n",
    "bipolar_df['sentiment_label'] = bipolar_df['compound'].apply(label_sentiment)\n",
    "bipolar_df['sentiment_label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f125f",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b37688",
   "metadata": {},
   "source": [
    "### TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e20863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmas_to_string(lemmas: list[str]):\n",
    "  return ' '.join(lemma for lemma in lemmas)\n",
    "\n",
    "bipolar_df['post_lemma_string_lowercase'] = bipolar_df['post_lemmas_lowercase'].apply(lemmas_to_string)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "  ngram_range=(2, 3),\n",
    "  min_df=5,\n",
    "  max_df=0.95,\n",
    "  stop_words=stop_words\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(bipolar_df['post_lemma_string_lowercase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3b3ab",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "\n",
    "model = NMF(\n",
    "  n_components=n_components,\n",
    "  init='random',\n",
    "  random_state=42,\n",
    "  max_iter=1000\n",
    ")\n",
    "\n",
    "W = model.fit_transform(X_tfidf) # document-topic matrix → each row shows how much each topic contributes to a post\n",
    "H = model.components_ # topic-term matrix → each row is a topic, each column a term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out() # Gets the words from the tf-idf vector\n",
    "\n",
    "for topic_idx, topic in enumerate(H): # for loop to map the top columns in the H matrix to their corresponding words in the feature_names\n",
    "  top_indices = topic.argsort()[::-1][:10] # reverse sort the top ten indices according to their topic weights\n",
    "  top_words = [feature_names[i] for i in top_indices] # retrieve the feature names for the top ten indices\n",
    "  print(f'Topic {topic_idx+1}: {\", \".join(top_words)}') # print the n-grams corresponding to each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54da09",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "  ngram_range=(2, 3),\n",
    "  min_df=5,\n",
    "  max_df=0.95,\n",
    "  stop_words=stop_words\n",
    ")\n",
    "\n",
    "X_count = count_vectorizer.fit_transform(bipolar_df['post_lemma_string_lowercase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43898644",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "  n_components=n_components,\n",
    "  random_state=42,\n",
    "  max_iter=50\n",
    ")\n",
    "\n",
    "lda.fit(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ee7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "  top_indices = topic.argsort()[::-1][:10]\n",
    "  top_words = [feature_names[i] for i in top_indices]\n",
    "  print(f'Topic {topic_idx + 1}: {\", \".join(top_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dfedb",
   "metadata": {},
   "source": [
    "### Top Posts and Sentiment per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_posts(topic_idx: int, n_posts: int):\n",
    "  topic_scores = W[:, topic_idx]\n",
    "  top_indices = topic_scores.argsort()[::-1][:n_posts]\n",
    "  return (pd.concat([bipolar_df.iloc[top_indices]['post'], sentiment_df.iloc[top_indices]['compound']], axis=1))\n",
    "\n",
    "for i in range(n_components):\n",
    "  posts = get_top_posts(i, 5)\n",
    "  print(f'Topic {i}:')\n",
    "  print(f\"{posts.apply(lambda x: 'Sentiment: ' + str(x['compound']) + '  Content: ' + str(x['post']), axis=1)}\")\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb0981",
   "metadata": {},
   "source": [
    "## Topic Modeling with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf501398",
   "metadata": {},
   "source": [
    "### Clustering Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f96e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "  sentences=bipolar_df['post_lemmas_lowercase'], # expects list of tokens\n",
    "  vector_size=100, # dimensionality of the word vectors\n",
    "  window=5, # context window size\n",
    "  min_count=5, # ignore words that appear fewer than 5 times\n",
    "  workers=4, # parallelization\n",
    "  seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fe852",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v_model.wv['manic']) # vector for the word 'bipoar'\n",
    "print(w2v_model.wv.most_similar('manic')) # words most similar to 'manic'\n",
    "print(w2v_model.wv.similarity('manic', 'depression')) # cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v_model.wv.key_to_index) # get all words in the vocabulary\n",
    "word_vectors = np.array([w2v_model.wv[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef04420",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "kmeans.fit(word_vectors)\n",
    "\n",
    "word_clusters = kmeans.labels_ # each word now has a cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53014387",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = defaultdict(list)\n",
    "\n",
    "for word, label in zip(words, word_clusters):\n",
    "  clusters[label].append(word)\n",
    "\n",
    "# print top words per cluster:\n",
    "for cluster_id, cluster_words in clusters.items():\n",
    "  print(f\"Cluster {cluster_id}: {', '.join(cluster_words[:10])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d5a4c",
   "metadata": {},
   "source": [
    "### Clustering Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec231673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a lightweight, general-purpose sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Your column is already a clean string version of the posts\n",
    "post_texts = bipolar_df['post_lemma_string_lowercase'].tolist()\n",
    "\n",
    "sentence_embeddings = model.encode(post_texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "\n",
    "kmeans_doc = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "kmeans_doc.fit(sentence_embeddings)\n",
    "\n",
    "bipolar_df['doc_topic'] = kmeans_doc.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55a368",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa169d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTopic(language=\"english\", verbose=True)\n",
    "topics, probs = model.fit_transform(bipolar_df['post_lemma_string_lowercase'], sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a8b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
